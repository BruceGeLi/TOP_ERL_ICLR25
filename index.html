<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="TOP-ERL-ICLR25">
  <meta property="og:title" content="TOP-ERL-ICLR25"/>
  <meta property="og:description" content="TOP-ERL-ICLR25"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/TOP_ERL_critic.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="400"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>TOP-ERL: Transformer-based Off-policy Episodic Reinforcement Learning (ICLR25 Spotlight)</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon_fire.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">TOP-ERL: Transformer-based Off-policy Episodic Reinforcement Learning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://brucegeli.github.io/" target="_blank">Ge Li*</a>,</span>
                  <span class="author-block">
                    Dong Tian,
                  </span>
                  <span class="author-block">
                    <a href="https://hongyizhoucn.github.io/" target="_blank">Hongyi Zhou,</a>
                  </span>
                  <span class="author-block">
                    <a href="https://xinkai-jiang.github.io/" target="_blank">Xinkai Jiang,</a>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.de/citations?user=hvjV43MAAAAJ&hl=de" target="_blank">Rudolf Lioutikov,</a>
                  </span>
                  <span class="author-block">
                    <a href="https://scholar.google.de/citations?user=GL360kMAAAAJ&hl=en" target="_blank">Gerhard Neumann,</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Karlsruhe Institute of Technology<br>ICLR 2025 SpotlightðŸ”¥</span>                    
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://openreview.net/pdf?id=N4NhVN30ph" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/BruceGeLi/TOP_ERL_ICLR25_Spotlight?tab=readme-ov-file" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
                
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- ERL -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/mp_demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        Episodic RL often uses the movement primitves (MPs) as a paramterized trajectory generator.  A simple illustration of using MPs is shown in the video.
      </h2>
    </div>
  </div>
</section>  


<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This work introduces Transformer-based Off-Policy Episodic Reinforcement Learning (TOP-ERL), a novel algorithm that enables off-policy updates in the ERL framework. In ERL, policies predict entire action trajectories over multiple time steps instead of single actions at every time step. These trajectories are typically parameterized by trajectory generators such as Movement Primitives (MP), allowing for smooth and efficient exploration over long horizons while capturing high-level temporal correlations. However, ERL methods are often constrained to on-policy frameworks due to the difficulty of evaluating state-action values for entire action sequences, limiting their sample efficiency and preventing the use of more efficient off-policy architectures. TOP-ERL addresses this shortcoming by segmenting long action sequences and estimating the state-action values for each segment using a transformer-based critic architecture alongside an n-step return estimation. These contributions result in efficient and stable training that is reflected in the empirical results conducted on sophisticated robot learning environments. TOP-ERL significantly outperforms state-of-the-art RL methods. Thorough ablation studies additionally show the impact of key design choices on the model performance.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Critic -->
<section class="hero teaser">
  <div class="container is-max-desktop">    
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3"><br>Transformer Critic for Action Sequence Value Estimation</h2>
    </div>      
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/Critic_animation_edit.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        In TOP-ERL, we utilize Transformers as an action sequence value estimator, to evaluate the value of executing a sequence of actions from an intermediate state in the episode. We train the critic via the N-step future returns shown below.
      </h2>
    </div>    
  </div>
</section>
<!-- End teaser video -->

<!-- N-step return -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/n_step_return.png" alt="N-step return"/>      
    </div>
  </div>
</section>
<!-- End teaser video -->  

<!-- Results -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <h2 class="title is-3"><br>Empirical Results</h2>
      
    </div>          
    <div class="hero-body">
      <img src="static/images/results_new.png" alt="N-step return"/>      
    </div>
  </div>
</section>
<!-- End teaser video -->    



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@inproceedings{
        li2025toperl,
        title={{TOP}-{ERL}: Transformer-based Off-Policy Episodic Reinforcement Learning},
        author={Ge Li and Dong Tian and Hongyi Zhou and Xinkai Jiang and Rudolf Lioutikov and Gerhard Neumann},
        booktitle={The Thirteenth International Conference on Learning Representations},
        year={2025},
        url={https://openreview.net/forum?id=N4NhVN30ph}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
